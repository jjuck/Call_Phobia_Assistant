{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline\n",
    "from transformers import ElectraTokenizerFast, ElectraModel, TFElectraModel\n",
    "from transformers import AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 엑셀 파일 불러오기\n",
    "# file_path = 'dataset.xlsx'\n",
    "# df = pd.read_excel(file_path)\n",
    "\n",
    "# # 피쳐와 타겟 추출\n",
    "# X = df['사람문장1']\n",
    "# y = df['감정_대분류']\n",
    "\n",
    "# # 데이터 확인\n",
    "# print(X.head())\n",
    "# print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'kobertdataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "X = df['발화문']\n",
    "y = df['상황']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happiness', 'neutral', 'sadness', 'angry', 'surprise', 'disgust',\n",
       "       'fear'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['상황'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emojis = ''.join(emoji.EMOJI_DATA.keys())\n",
    "# pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "# url_pattern = re.compile(\n",
    "#     r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "# def clean(x): \n",
    "#     x = pattern.sub(' ', x)\n",
    "#     x = emoji.replace_emoji(x, replace='')\n",
    "#     x = url_pattern.sub('', x)\n",
    "#     x = x.strip()\n",
    "#     x = repeat_normalize(x, num_repeats=2)\n",
    "#     return x\n",
    "\n",
    "# X = X.apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = T5ForConditionalGeneration.from_pretrained('j5ng/et5-typos-corrector')\n",
    "# tokenizer = T5Tokenizer.from_pretrained('j5ng/et5-typos-corrector')\n",
    "\n",
    "# typos_corrector = pipeline(\n",
    "#     \"text2text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0 if torch.cuda.is_available() else -1,\n",
    "#     framework=\"pt\",\n",
    "# )\n",
    "\n",
    "# X = X.apply(lambda x: typos_corrector(x,\n",
    "#             max_length=128,\n",
    "#             num_beams=5,\n",
    "#             early_stopping=True)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ElectraTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = ElectraTokenizerFast.from_pretrained(\"kykim/electra-kor-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"kykim/electra-kor-base\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\", num_labels=6)\n",
    "# model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import FunnelTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = FunnelTokenizerFast.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"kykim/funnel-kor-base\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"kykim/bert-kor-base\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ElectraTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "# tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\USER\\Desktop\\beomi.pth'\n",
    "model = torch.load(PATH, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 2/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 3/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 4/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 5/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# 데이터 전처리 및 토큰화\n",
    "X_list = X.values.tolist()\n",
    "sequences = tokenizer(X_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences['input_ids'], y_encoded, test_size=0.2, random_state=42)\n",
    "X_train_mask, X_test_mask, _, _ = train_test_split(sequences['attention_mask'], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train, X_train_mask, y_train = X_train.to(device), X_train_mask.to(device), torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test, X_test_mask, y_test = X_test.to(device), X_test_mask.to(device), torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, X_train_mask, y_train)\n",
    "test_dataset = TensorDataset(X_test, X_test_mask, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    # model.train()\n",
    "    # for input_ids, attention_mask, labels in train_loader:\n",
    "    #     input_ids = input_ids.to(device)\n",
    "    #     attention_mask = attention_mask.to(device)\n",
    "    #     labels = labels.to(device)\n",
    "        \n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    #     logits = outputs.logits\n",
    "    #     loss = criterion(logits, labels)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_losses.append(outputs.loss.item())\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc = (preds == labels).float().mean().item()\n",
    "            val_accs.append(acc)\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = np.mean(val_accs)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'beomi.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 2, 3, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        happiness\n",
       "1        happiness\n",
       "2        happiness\n",
       "3        happiness\n",
       "4          neutral\n",
       "           ...    \n",
       "19369         fear\n",
       "19370        angry\n",
       "19371         fear\n",
       "19372    happiness\n",
       "19373      disgust\n",
       "Name: 상황, Length: 19374, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to Number Mapping: {'angry': 0, 'disgust': 1, 'fear': 2, 'happiness': 3, 'neutral': 4, 'sadness': 5, 'surprise': 6}\n",
      "Number to Label Mapping: {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happiness', 4: 'neutral', 5: 'sadness', 6: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 예시 레이블 리스트 (학습 시 사용한 레이블 리스트와 동일해야 합니다)\n",
    "emotion_labels = ['happiness', 'neutral', 'sadness', 'angry', 'surprise', 'disgust', 'fear']\n",
    "\n",
    "# LabelEncoder 인스턴스 생성 및 레이블 피팅\n",
    "le = LabelEncoder()\n",
    "le.fit(emotion_labels)\n",
    "\n",
    "# 레이블과 숫자 매핑 출력\n",
    "label_to_number = {label: idx for label, idx in zip(le.classes_, le.transform(le.classes_))}\n",
    "number_to_label = {idx: label for label, idx in label_to_number.items()}\n",
    "\n",
    "print(\"Label to Number Mapping:\", label_to_number)\n",
    "print(\"Number to Label Mapping:\", number_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 2/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 3/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 4/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "Epoch 5/5, Validation Loss: 0.30044952777205725, Validation Accuracy: 0.9197530864197531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.86      0.91      0.89       655\n",
      "     disgust       0.93      0.88      0.90       465\n",
      "        fear       0.94      0.95      0.95       281\n",
      "   happiness       0.95      0.94      0.94       949\n",
      "     neutral       0.91      0.93      0.92       634\n",
      "     sadness       0.90      0.88      0.89       530\n",
      "    surprise       0.97      0.95      0.96       361\n",
      "\n",
      "    accuracy                           0.92      3875\n",
      "   macro avg       0.92      0.92      0.92      3875\n",
      "weighted avg       0.92      0.92      0.92      3875\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.86      0.91      0.89       655\n",
      "     disgust       0.93      0.88      0.90       465\n",
      "        fear       0.94      0.95      0.95       281\n",
      "   happiness       0.95      0.94      0.94       949\n",
      "     neutral       0.91      0.93      0.92       634\n",
      "     sadness       0.90      0.88      0.89       530\n",
      "    surprise       0.97      0.95      0.96       361\n",
      "\n",
      "    accuracy                           0.92      3875\n",
      "   macro avg       0.92      0.92      0.92      3875\n",
      "weighted avg       0.92      0.92      0.92      3875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 라벨 인코딩\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# 데이터 전처리 및 토큰화\n",
    "X_list = X.values.tolist()\n",
    "sequences = tokenizer(X_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences['input_ids'], y_encoded, test_size=0.2, random_state=42)\n",
    "X_train_mask, X_test_mask, _, _ = train_test_split(sequences['attention_mask'], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train, X_train_mask, y_train = X_train.to(device), X_train_mask.to(device), torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test, X_test_mask, y_test = X_test.to(device), X_test_mask.to(device), torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, X_train_mask, y_train)\n",
    "test_dataset = TensorDataset(X_test, X_test_mask, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_losses.append(outputs.loss.item())\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc = (preds == labels).float().mean().item()\n",
    "            val_accs.append(acc)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = np.mean(val_accs)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 클래스별 정확도 출력\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=le.classes_)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906075"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.8811 + 0.9537 + 0.8796 + 0.9099) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9322"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise Accuracy:\n",
      "angry: 0.9099\n",
      "disgust: 0.8796\n",
      "fear: 0.9537\n",
      "happiness: 0.9368\n",
      "neutral: 0.9322\n",
      "sadness: 0.8811\n",
      "surprise: 0.9501\n"
     ]
    }
   ],
   "source": [
    "# 클래스별 정확도 계산\n",
    "class_accuracies = {}\n",
    "for i, class_label in enumerate(le.classes_):\n",
    "    true_positives = conf_matrix[i, i]\n",
    "    total_samples = conf_matrix[i, :].sum()\n",
    "    class_accuracies[class_label] = true_positives / total_samples\n",
    "\n",
    "print(\"Class-wise Accuracy:\")\n",
    "for class_label, accuracy in class_accuracies.items():\n",
    "    print(f\"{class_label}: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
